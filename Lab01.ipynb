{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebraic graph theory\n",
    "Algebraic graph theory is concerned with the study of graphs through several related matrices.\n",
    "We then need to understand how arrays are represented and can be manipulated with Python using the library NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array creation\n",
    "You can create a numpy array from a Python list or tuple using the  `array ` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([2,3,4])\n",
    "print(\"a=\", a)\n",
    "print(\"Dimension of a:\", a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `array ` transforms sequences of sequences into two-dimensional arrays, i.e., matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([(1.5,2,3), (4,5,6)])\n",
    "print(b)\n",
    "print(\"Dimension of b:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function  `zeros ` creates an array full of zeros, the function  `ones ` creates an array full of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros((3, 4))) # tuple (3,4) is the shape of the zero function to be created\n",
    "print(np.ones((2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can create matrices whose elements space in a given range using  `arange `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first two arguments are the starting point (included) and the ending point (excluded)\n",
    "# the third argument is the step-size\n",
    "np.arange( 10, 30, 5 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or using the function `linspace` that receives as an argument the number of elements that we want to obtain in the given range, instead of the step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first two arguments are the starting and ending point (both included!), \n",
    "# third argument is the number of elements\n",
    "np.linspace( 0, 2, 9 )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, numpy `ndarray` can represent arrays of any dimension but we will restrict to dimension 1 (vectors) and 2 (matrices). One-dimensional arrays are printed as rows, bidimensionals as matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(): default start value is 0, default step size is 1\n",
    "a = np.arange(6)                         # 1d array\n",
    "print(\"a:\", a)\n",
    "b = np.arange(12).reshape(4,3)           # 2d array\n",
    "print(\"b:\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations\n",
    "Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array( [20,30,40,50] )\n",
    "print(\"a=\",a)\n",
    "# np.arange(): default start value is 0, default step size is 1, \n",
    "# so b = [0,1,2,3]\n",
    "b = np.arange( 4 )\n",
    "print(\"b=\", b)\n",
    "c = a-b\n",
    "print(\"c= a-b =\", c)\n",
    "# ** denotes the second power ^2\n",
    "print(\"b^2=\", b**2) \n",
    "print(\"10 sin(a)=\", 10*np.sin(a))\n",
    "print(\"a<35:\", a<35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product operator `*` operates elementwise in NumPy arrays. \n",
    "The matrix product can be performed using the `@` operator or the `dot` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two numpy arrays starting from two (python) arrays of arrays\n",
    "A = np.array( [[1,1],   \n",
    "               [0,1]] )\n",
    "B = np.array( [[2,0],\n",
    "               [3,4]] )\n",
    "print(\"A*B=\",A * B )                      # elementwise product\n",
    "print(\"A@B=\",A @ B  )                     # matrix product\n",
    "print(\"A.dot(B)=\",A.dot(B)  )                  # another matrix product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called “universal functions”(`ufunc`). Within NumPy, these functions operate elementwise on an array, producing an array as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.arange(3) # B=[0,1,2]\n",
    "np.exp(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral graph theory\n",
    "We will explore several notions from spectral graph theory by analysing the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(1,11))\n",
    "nx.add_cycle(G,[1,2,3])\n",
    "nx.add_cycle(G,[4,5,6])\n",
    "nx.add_cycle(G,[8,9,10])\n",
    "G.add_edges_from([(4,3), (3,8), (5,7), (7,7)])\n",
    "\n",
    "# define pos according to spring layout\n",
    "# to fix nodes' positions in all graph drawings.\n",
    "# spring_layout positions nodes using Fruchterman-Reingold force-directed algorithm.\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G,pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant probability distributions\n",
    "In this section we show how to extract the characteristic matrices of a graph and how to compute the invariant distribution of it.\n",
    "\n",
    "First we construct the weight matrix (called adjacency matrix in NetworkX, even if the graph is weighted) W, the degree matrix D and the normalized weight matrix P:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nx.adjacency_matrix(G) # -> returns type is scipy.sparse.csr_matrix\n",
    "# convert W to a numpy array\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "print(\"Degrees\",degrees)\n",
    "D = np.diag(degrees)\n",
    "print(\"D:\",D)\n",
    "# P = D^(-1) W\n",
    "P = np.linalg.inv(D) @ W \n",
    "print(\"P:\",P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the invariant distributions of G, i.e., the normalized eigenvectors of P' relative to eigenvalue 1, which solve\n",
    "$$\n",
    "P' \\pi = \\pi, \\quad \\sum_i \\pi_i = 1\n",
    "$$ To do this, first we compute all eigenvalues and eigenvectors of P' with function `np.linalg.eig()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eig(P.T) returns the eigenvalues (in vector w)\n",
    "# and eigenvectors (in matrix v) of P'\n",
    "w,v = np.linalg.eig(P.T)\n",
    "print(\"eigenvalues:\",w) # -> the 0th and 5th eigenvalues are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.round(decimals=3).real[0]==1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two eigenvalues equal 1 (in position 0 and 5 of `w`). We select the eigenvectors corresponding to the two occurrencies of eigenvalue 1 and we normalize them to obtain the two invariant distributions (`pi0` and `pi5`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we iterate over indices corresponding to eigenvalues 1\n",
    "# i.e. corresponding to entries of w that are equal 1:\n",
    "# for each index we extract the corresponding eigenvector in v\n",
    "# and normalize it\n",
    "\n",
    "# we use np.isclose() to compare eigenvalues to 1 to avoid\n",
    "# numerical precision errors\n",
    "for index in [i for i in range(len(G)) if np.isclose(w[i],1)]: \n",
    "    pi = v[:,index].real  # -> eigenvectors are complex but pi is real, so we convert it to real\n",
    "    pi = pi/np.sum(pi)\n",
    "    print(\"pi\", index, \"=\", pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Perform the computation of the extremal eigenvectors of G looking at its condensation graph.\n",
    "\n",
    "1. Find the attractive components of G\n",
    "2. For each attractive component, construct the corresponding induced subgraph\n",
    "3. Compute the P matrix of each sink and its invariant measure\n",
    "4. Map the obtained measures back to the original graph G (by adding zeros in the appropriate positions)\n",
    "\n",
    "**Hint**: exploit previous code and the code from Lab00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we have exploited the function `numpy.linalg.eig` to compute all the eigenvalues and eigenvectors of P', and we have selected the leading ones. \n",
    "Another option is to apply an iterative method which converges to the leading eigenvector.\n",
    "\n",
    "To see this, consider the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.lollipop_graph(3,2).to_directed()\n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G is strongly connected and aperiodic, as we can check calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is strongly connected:\", nx.is_strongly_connected(G))\n",
    "print(\"Is aperiodic:\", nx.algorithms.dag.is_aperiodic(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, as you will study, the flow dynamics defined as\n",
    "$$\n",
    "\\begin{cases} \n",
    "x(t+1) = P'x(t) \\\\\n",
    "x(0) = x_0\n",
    "\\end{cases}\n",
    "$$\n",
    "is guardanteed to converge. The limit of such dynamics is proportional to the invariant distribution of G, more precisely\n",
    "$$\n",
    "\\lim_{t \\to \\infty} x(t) =\\pi \\mathbf{1}'x_0 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by setting $ x_0 = \\frac{1}{n} \\mathbf{1} $ we obtain that\n",
    "$$\n",
    "\\lim_{t \\to \\infty} x(t) =\\pi \n",
    "$$\n",
    "so that $x(t)$ approximates $\\pi$ as $t$ grows larger.\n",
    "Then we can compute $\\pi$ by an iterative implementation of the flow dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the relevant matrices \n",
    "\n",
    "W = nx.adjacency_matrix(G) # -> returns type is scipy.sparse.csr_matrix\n",
    "# convert W to a numpy array\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# Simulate the evolution of the flow dynamics to approximate its limit\n",
    "\n",
    "n_nodes = len(G)\n",
    "# initial condition: 1/n-uniform vector of size n_nodes\n",
    "x_0 = np.ones((n_nodes,1))/n_nodes\n",
    "# set a tolerance to assess convergence to the limit\n",
    "tol = 1e-5\n",
    "# evolve the flow dynamics multiplying by P'\n",
    "x_old = x_0\n",
    "while True:\n",
    "    x_new = P.T @ x_old\n",
    "    if np.linalg.norm(x_new-x_old) < tol:\n",
    "        break\n",
    "    x_old=x_new\n",
    "pi_approx = x_new\n",
    "print(\"Approximation of pi:\", pi_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the accuracy of the approximation by comparing it to the invariant distribution found using the `numpy.linalg.eig` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the invariant distribution using eig()\n",
    "\n",
    "values,vectors = np.linalg.eig(P.T)\n",
    "index = np.argmax(values.real)\n",
    "pi = vectors[:,index].real\n",
    "pi = pi/np.sum(pi)\n",
    "print(\"pi=\", pi)\n",
    "\n",
    "# Compute the approximation error as the difference between the two results\n",
    "# pi_approx is a column, i.e. a 2 dimentional ndarray of shape (5,1), while pi is a row. \n",
    "# To take the difference we uniform shapes with .T (transpose)\n",
    "error = np.linalg.norm(pi_approx.T-pi) \n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Adapt the previous iterative procedures to compute for the graph G\n",
    "1. the Katz centrality \n",
    "$ z =  \\frac{1-\\beta}{\\lambda_W} W' z + \\beta \\mu $\n",
    "with $\\mu = \\mathbf{1}$ and $\\beta=0.15$\n",
    "2. and Bonachich centrality \n",
    "$ x = (1-\\beta)P' x + \\beta \\mu $\n",
    "with $\\mu = \\mathbf{1}$ and $\\beta=0.15$ \n",
    "    - Check the result of point 2. using `networkx.algorithms.link_analysis.pagerank_alg.pagerank` (the Page Rank centrality is a special case of the Bonachich centrality where $\\beta=0.15$ and $\\mu = \\mathbf{1}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing centralities on graphs\n",
    "We can compute centralities both using NetworkX algorithms and performing iterative procedures. It is important to make sense of the centrality vectors, and a usefull way to do this is by visualizing centralities on graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small networks Example: Zachary's Karate Club\n",
    "Zachary's Karate Club network is a well-know network example. This is a quite small network so we can compute centralities direcly. To better understand the meaning of centrality measure it is usefull to visualize them by producing appropriate graph representations.\n",
    "\n",
    "Let's first load and visualize Zachary's Karate Club network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "pos = nx.spring_layout(G) # Fix node positions on all pictures according to spring layout\n",
    "nx.draw_networkx(G, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute, for example, the degree centrality of G. The following code shows how to represent G so that nodes size and color reflects their centrality value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Degree centrality\n",
    "\n",
    "# dc is a dictionary with nodes as keys and degree centralities as values\n",
    "dc = nx.degree_centrality(G) \n",
    "plt.figure(1, figsize=(10,7))\n",
    "nx.draw(G,pos,\n",
    "         with_labels=True,\n",
    "         # keys of dc are nodes\n",
    "         nodelist=dc.keys(), \n",
    "         # node size is proportional to centrality value\n",
    "         node_size = [d*7000 for d in dc.values()], \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=list(dc.values()),\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this proceadure with a different measure, eigenvector centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvector centrality\n",
    "ec = nx.eigenvector_centrality(G)\n",
    "plt.figure(1, figsize=(10,7))\n",
    "nx.draw(G, pos,\n",
    "          with_labels=True,\n",
    "          nodelist=ec.keys(),\n",
    "          # node size is proportional to eigenvector centrality\n",
    "          node_size = [d*3000 for d in ec.values()],  \n",
    "          node_color=list(ec.values()),\n",
    "          font_size=8,\n",
    "          cmap=plt.cm.Reds,\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to compare different centrality measures for the same graph and see how they are correlated. Below we visualize the correlation between degree centrality and eigenvector centrality of G:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation degree-eigenvector\n",
    "\n",
    "# x corresponds to degree centrality values\n",
    "xdata = list(dc.values()) \n",
    "# y corresponds to eigenvector centrality values\n",
    "ydata = list(ec.values()) \n",
    "\n",
    "plt.figure(1, figsize=(7,7))\n",
    "# for each node, we plot an \"+\" with coordinates equal to the values of its\n",
    "# degree and eigenvector centrality\n",
    "plt.plot(xdata,ydata, '+') \n",
    "plt.xlabel('Degree Centrality')\n",
    "plt.ylabel('Eigenvector Centrality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two centralities appear to be correlated for G. To explore this in more details it is usefull to add node ids, so that we can see which are the nodes with higher or lower correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding node ids:\n",
    "\n",
    "# We define a figure and we construct two subplots: \n",
    "# on the left we plot the centralities correlation diagram\n",
    "# with node labels, on the right we draw the graph \n",
    "# with same node labels\n",
    "fig = plt.figure(1, figsize=(14,7))\n",
    "# add_subplot() returns the axes of the subplot\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "\n",
    "for v in range(len(dc)):\n",
    "    # Axes.text() add the text s to Axes instance (i.e., to the subplot)\n",
    "    # at location x, y. For each node v we plot \n",
    "    # node ids in position (xdata[v], ydata[v]) where xdata = list(dc.values())\n",
    "    # and ydata = list(ec.values())\n",
    "    ax1.text(x = xdata[v], y = ydata[v], s=str(v))\n",
    "# we set the limits for x and y scales\n",
    "ax1.set_xlim(0, 0.6)\n",
    "ax1.set_ylim(0, 0.4)\n",
    "ax1.set_xlabel('Degree Centrality')\n",
    "ax1.set_ylabel('Eigenvector Centrality')\n",
    "\n",
    "nx.draw_networkx(G, pos, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Compare Katz and Bonacich centralities for G, reusing the code you made in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing sensitivity of measures\n",
    "In this section we will check the dependence of centrality measures with respect to their paramenters and the sensitivity of the iterative algorithms to compute such measure with respect to the number of iterations.\n",
    "\n",
    "## The effect of parameters\n",
    "In our first experiment we analyze the dependence of Page Rank centrality on the parameter $\\alpha$. We set any 3 distinct values for $\\alpha$ while we fix the number of iterations, and run Page Rank. Then we plot the resulting Page Rank values with respect to $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# we consider values for alpha from 0.1 to 0.9 with step size 0.2\n",
    "alphas = np.arange(0.1, 0.9, 0.2)\n",
    "\n",
    "for alp in alphas:\n",
    "    # pagerank has parameters alpha and mu:\n",
    "    # note that alpha = 1-beta and weight parameters mu are set to 1 by default\n",
    "    pr = nx.pagerank(G, alpha=alp) \n",
    "    prval = list(pr.values())\n",
    "    ax.plot(prval, color=np.random.rand(3), label='alpha {0:.2f}'.format(alp))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "1. explain the previous result. \n",
    "\n",
    "**Hint:** keep in mind that the parameter alpha used by `nx.pagerank` corresponds to $1-\\beta$, where $\\beta$ has the same meaning as in the lectures.\n",
    "2. Repeat the analysis. This time keep alpha fixed to 0.5 and select 3 different non-uniform vectors $\\mu$ as `weight` parameter to `pagerank`. How do you interpret the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of iteration number\n",
    "In this section we consider a bigger network and we analyse the speed of convergence of iterative algorithms for computing centrality measures. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [political blogs](http://networkdata.ics.uci.edu/data/polblogs/polblogs.gml) network (save it as a .gml file in the working directory of this notebook) and import it as a Graph object. We check the basic properties of G.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gml('polblogs.gml')\n",
    "print(\"Type of G:\", type(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G is a directed multigraph due to the presence of parallel links. To compute centrality measures we convert G to a directed graph by collapsing parallel edges summing their unitary weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GG = nx.Graph()\n",
    "for n, nbrs in G.adjacency():\n",
    "    # edict is a dictionary of dictionaries; \n",
    "    # the keys of edict are parallel edges from n to nbr;\n",
    "    # the values of edict are dictionary,\n",
    "    # containing attribute values of the corresponding edge\n",
    "    for nbr, edict in nbrs.items(): \n",
    "        # each edge has weight=1, so total value is just  \n",
    "        # the number of parallel edges\n",
    "        total_value = len(edict) \n",
    "        GG.add_edge(n, nbr, weight = total_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the convergence speed of `nx.pagerank` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(14,7))\n",
    "\n",
    "# set 3 iteration numbers\n",
    "iters = [10,15,50]\n",
    "# define the position of the next plot in the subplot grid\n",
    "position = 1\n",
    "# create a list to collect the page rank values obtained in the three runs \n",
    "prvals = []\n",
    "\n",
    "for max_iter in iters:\n",
    "    pr = nx.pagerank(GG, max_iter = max_iter) \n",
    "    # compute page rank values\n",
    "    prval = list(pr.values())\n",
    "    # append the result to the list\n",
    "    prvals.append(np.array(prval)) \n",
    "    # create a new sublot in the grid\n",
    "    ax = fig.add_subplot(2,2,position)\n",
    "    # plot the PR values\n",
    "    ax.plot(prval, color=np.random.rand(3), label='{0:d} iterations'.format(max_iter))\n",
    "    position+=1\n",
    "\n",
    "# add a legend which contains all label\n",
    "# informations specified in previous plot calls\n",
    "fig.legend()  \n",
    "# we assume the values obtained with nx.pagerank()\n",
    "# with no iterations constraints as a benchmark\n",
    "benchmark = np.array(list(nx.pagerank(GG).values())) \n",
    "# we compute errors as norm of the differences wrt the benchmark\n",
    "errors = [np.linalg.norm(prval-benchmark) for prval in prvals]\n",
    "print(\"Errors:\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nx.pagerank` algorithm converges very fast! \n",
    "### Exercise\n",
    "Check if your iterative algorithm for computing Bonachich centrality is as good as this by performing a similar analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
